{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A00240 - SAS Certification Prep Guide Statistical Business Analysis Using SAS 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics 1: Introduction to ANOVA, Regression, and Logistic Regression\n",
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Modeling: Types of Variables\n",
    "\n",
    "In statiscal modelling the variables could be continuos, categorial u ordinal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictor Variables: \n",
    "* Input\n",
    "* Exploratory \n",
    "* Independent (design experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response variables: \n",
    "* Outcome\n",
    "* Target\n",
    "* dependent (design experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Models\n",
    "\n",
    "#### General Linear Model (GLM): \n",
    "is the reference to differents type of analysis\n",
    "\n",
    "* Response variable is continuos\n",
    "* the error's distribution is normal\n",
    "\n",
    "#### GLM - ANOVA\n",
    "how the changes in the leves of the predictors affect the change in the response\n",
    "\n",
    "* the response varible is continuos \n",
    "* all predictor variables are categorical\n",
    "\n",
    "#### GLM - Ordinary Least Squares Regression\n",
    "\n",
    "* the response varible is continuos \n",
    "* all predictor variables are continuos\n",
    "\n",
    "#### GLM - Logistic Regression\n",
    "\n",
    "* the response varible is binary \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanatory versus Predictive Modeling\n",
    "\n",
    "#### Explanatory (Inferential statistics)\n",
    "Conclutions about a population based from a sample of that population. The goal is to develop a model that answers the question, \"How is X related to Y?\".\n",
    "* accurately estimating model parameters\n",
    "* assess this using p-values and confidence intervals\n",
    "* have small sample sizes\n",
    "* few variables\n",
    "\n",
    "#### Predictive Modeling\n",
    "Predicts values of the response variable base on the existing values of the predictors variables. The goal is make accurate predictions \n",
    "\n",
    "* large sample sizes\n",
    "* many variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Review of Statistical Concepts\n",
    "\n",
    "#### P-value\n",
    "The probability of obtaining a test statistic as extreme or more extreme than the one observed in your data given that the null hypothesis is true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Sample t Tests\n",
    "\n",
    "#### Performing a t Test\n",
    "* a one-sample t test compares the mean calculated from a sample to a hypothesized mean. \n",
    "* you must use Student's t distribution, rather than the normal distribution, for calculating p-values and confidence limits.\n",
    "* Student's t distribution is similar to the normal distribution, but it has more probability in the tails and is not as peaked as the normal distribution. Student's t distribution approaches the normal distribution as the sample size increases.\n",
    "\n",
    "\n",
    "\n",
    "#### Two-Sample t Tests (proc ttest)\n",
    "* the observations are independent.\n",
    "* the are normally distributed populations \n",
    "* have equal population variances\n",
    "* In SAS procedure, the class variable has to have only two levels, the populations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ANOVA and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Analysis of Associations\n",
    "\n",
    "Graphical exploration of associations between predictors and the response can guide model building. Interesting relationships that would be otherwise missed (for example, quadratic relationships or interactions) can present themselves in plots and can be incorporated into models.\n",
    "Variables that are clearly unrelated to the response variable could potentially be removed from the modeling effort. Any information to help guide the decision to include or exclude variables can simplify the process of choosing appropriate predictors for your model.\n",
    "\n",
    "#### Identifying Associations in ANOVA with Box Plots\n",
    "An association exists between two variables when the expected value of one variable changes at different levels of the other variable. if the line that connects the mean in the box is not horizontal it suggest an association.\n",
    "\n",
    "#### Identifying Associations in Linear Regression with Scatter Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Way ANOVA\n",
    "\n",
    "#### The ANOVA Hypothesis\n",
    "The ANOVA test helps you determine whether the differences are large enough to indicate that the population means are different.\n",
    "\n",
    "#### The ANOVA Model\n",
    "assumptions: \n",
    "* independent observations. Independence implies that the errors, Îµik, in the model are uncorrelated. Good data collection designs can help ensure this assumption.\n",
    "* the error terms are normally distributed for every group or treatment. \n",
    "* the third assumption is that the error terms have equal variances across treatments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA Post Hoc Tests (multiple-comparison procedures)\n",
    "will determine which pairs of groups are significantly different from each other. \n",
    "\n",
    "#### Multiple Comparison Methods\n",
    "\n",
    "#### Tukey's and Dunnett's Multiple Comparison Methods\n",
    "\n",
    "The Tukey method, or honest significant difference test, \n",
    "* conducts all possible pairwise comparisons and controls the EER (experimentwise error rate) to equal the alpha level that you specify. \n",
    "\n",
    "Dunnett's method \n",
    "* is a specialized multiple comparison test that enables you to compare a single control group, such as a placebo in a drug trial, to all other groups or treatments. \n",
    "\n",
    "#### Diffograms and Control Plots\n",
    "You can use diffograms to visually assess whether pairs of group means are statistically different.\n",
    "A control plot displays the least squares mean and decision limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation\n",
    "to use correlation analysis to test for linear associations among the continuous variables.Because correlation is a measure of the linear association between two variables, identifying correlations provides information about how well a continuous predictor will explain the response within a regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple linear regression \n",
    "can be used to more fully describe the relationship between two continuous variables, as opposed to scatter plots and Pearson correlations. Regression model parameter estimates not only define the line of best fit corresponding to the linear association between variables, but they also describe how a change in a predictor corresponds to a change in the response.\n",
    "\n",
    " the goal is to identify the equation that characterizes the linear association between the predictor variable and the response variable, and use the model to then estimate the response for a given value of the predictor.\n",
    "\n",
    "\n",
    "#### Hypothesis Testing and Assumptions for Linear Regression\n",
    "Assumptions: \n",
    "* the mean of the response variable is linearly related to the value of the predictor variable. \n",
    "* the error terms are normally distributed with a mean of 0\n",
    "* the error terms have equal variances\n",
    "* the error terms are independent at each value of the predictor variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. More Complex Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyzes the effect of each predictor individually, and tests for interactions between them.\n",
    "\n",
    "#### Applying the Two-Way ANOVA Model\n",
    "\n",
    "In a two-way ANOVA, there are two variables, or factors, which affect the response variable. Each factor has multiple levels. Treatments, or treatment groups, are formed by combining the two factors. \n",
    "\n",
    "\n",
    "There are three assumptions: \n",
    "* The observations are independent. \n",
    "* The data is normally distributed. \n",
    "* The population variances are equal for each treatment combination.\n",
    "\n",
    "#### Interactions\n",
    "An interaction occurs when the differences between group means of one variable change at different levels of another variable. \n",
    "This is generally the method that you use when you analyze designed experiments. But even in designed experiments, some statisticians suggest that if the interaction is not significant, you can delete it from your model, rerun the model, and then analyze the main effects. This increases the power of the main effects test. \n",
    "\n",
    "### Multiple Regression\n",
    "if you want to model the relationship between the response variable and more than one predictor variable?\n",
    "\n",
    "#### Hypothesis Testing for Multiple Regression\n",
    "The hypotheses for multiple regression are similar to those for simple linear regression. The null hypothesis is that the multiple regression model does not fit the data better than the baseline model (a horizontal regression surface with no tilt in space). The alternative hypothesis is that the regression model fits the data better than the baseline model.\n",
    "\n",
    "assumptions: \n",
    "* The response, Y, is accurately modeled by a linear function of the Xs. \n",
    "* The random error term, Îµ : \n",
    "  * has a normal distribution with a mean of zero. \n",
    "  * has a constant variance, Ï2, also known as homoscedasticity. \n",
    "  * The errors are independent. \n",
    "\n",
    "#### Multiple Linear Regression versus Simple Linear Regression\n",
    "\n",
    "Why would you perform multiple linear regression instead of a series of simple linear regressions? The biggest advantage is that multiple regression enables you to determine the relationship between a predictor and response while controlling for all other predictors included in the model.Sometimes a hidden relationship can be revealed or a strong relationship can disappear when additional predictors are accounted for by including them in the regression model. You can determine whether a relationship exists between the response variable and several predictor variables simultaneously. You can also test for interactions just like in ANOVA.\n",
    "\n",
    "Disadvantage,  The increased complexity makes it more difficult to interpret the models, and to decide which model to use.\n",
    "\n",
    "\n",
    "When do you use multiple regression? It's actually a powerful tool for both explanatory analysis and for prediction.\n",
    "\n",
    "\n",
    "####  Adjusted R square\n",
    "\n",
    "The adjusted R square is like R-square, but it considers the number of terms in the model in addition to model fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Model Building and Effect Selection\n",
    "\n",
    "With k predictors, there are 2**k possible models. \n",
    "\n",
    "you can use several techniques for model selection to help you produce a manageable number of candidate models. Then you can use your subject-matter expertise and knowledge of the question to choose the best model for your purpose. Produce candidate models using stepwise model selection techniques, including forward, backward, and stepwise selection.\n",
    "\n",
    "#### The All-Possible Regressions Approach to Model Building\n",
    "For k predictors, there are 2k possible models, including the intercept-only model.  when you use the all-possible regressions approach, SAS calculates all possible regression models. However, you can reduce the number of models in the output by specifying the BEST= option in the MODEL statement of PROC REG. SAS will still evaluate all possible models, but display only the requested subset. \n",
    "\n",
    "\n",
    "### The Stepwise Selection Approach to Model Building\n",
    "Stepwise selection methods include forward, backward, and stepwise, and you'll use these approaches to select variables based on their p-values.\n",
    "\n",
    "#### Forward selection\n",
    "starts with no predictor variables in the model. This method computes an F statistic for each predictor variable not in the model, and examines the largest of these statistics. If it's significant at a specified significance level, the corresponding variable is added to the model. After a variable is added to the model, it stays in, even if it becomes non-significant later. Forward selection keeps adding variables, one at a time, until none of the remaining variables meets the specified level for entry, **0.50 by default**.\n",
    "\n",
    "#### Backward selection, \n",
    "also called backward elimination, starts with all predictor variables in the model. Results of the F test for individual parameter estimates are examined, and the least significant variable that is above the specified significance level is removed. After a variable is removed from the model, it remains excluded and cannot reenter. Backward selection is repeated until no other variable in the model meets the specified significance level for removal, **0.10 by default**.\n",
    "\n",
    "#### Stepwise selection\n",
    "combines aspects of both forward and backward selection. It starts with no predictor variables in the model and incrementally builds a model one variable at a time, as in forward selection. However, as in backward selection, stepwise selection can drop non-significant variables. The stepwise selection process terminates if no further variables can be added to or removed from the model, or when the variable to be added to the model is the one just deleted from it. **The default p-values to enter and stay in the model are both 0.15**. The default p-values to enter or stay in a model for all stepwise selection techniques can be changed based on the research goal or subject-matter expertise.\n",
    "\n",
    "#### Interpreting p-Values and Parameter Estimates\n",
    "\n",
    "\n",
    "### Information Criterion and Other Selection Options\n",
    "Four types of information criteria are available in PROC GLMSELECT: \n",
    "* Akaike's information criterion (AIC), \n",
    "* corrected Akaike's information criterion (AICC), \n",
    "* Sawa Bayesian information criterion (BIC)\n",
    "* Schwarz Bayesian information criterion (SBC).\n",
    "\n",
    "Each information criterion searches for a model that minimizes the unexplained variability with as few effects in the model as possible. In other words, they search for the most parsimonious model.The model with the smaller information criterion is considered better. The information criteria tells us only which one is relatively better.\n",
    "\n",
    "#### Adjusted R-Square and Mallows' Cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Model Post-Fitting for Inference\n",
    "to verify these assumptions and diagnose problems that you encounter in linear regression. You'll learn to examine residuals, identify outliers that are numerically distant from the bulk of the data, and identify influential observations that unduly affect the regression model. Finally, you learn to diagnose collinearity to avoid inflated standard errors and parameter instability in the model.\n",
    "\n",
    "### Influential Observations\n",
    "\n",
    "to detect outliers\n",
    "* use STUDENT residuals. \n",
    "\n",
    "To detect influential observations\n",
    "* use **Cook's D statistics** (best for explanatory models): Cook's D is a measure of the simultaneous change in all parameter estimates when an observation is deleted. The horizontal line shows the Cook's D cutoff boundary. \n",
    "* **RSTUDENT** residuals, \n",
    "* **DFFITS** statistics.(most useful for predictive models): DFFITS measures the impact that an observation has on the predicted value. This plot flags several observations as influential points based on DFFITS.\n",
    "\n",
    "To determine which predictor variable is being influenced, \n",
    "* you can use **DFBETAS**:  The DFBETAS plot is a panel plot, which contains one plot for each parameter. In this case, SAS created two panels. Each plot labels the points that are potentially influencing the parameter that's associated with each of the predictor variables.\n",
    "\n",
    "#### Handling Influential Observations\n",
    "What do you do with influential observations? \n",
    "* First, recheck for data entry errors and correct them if possible.\n",
    "* Second, if the data appears to be valid, consider whether you have an adequate model. A different model might fit the data better.\n",
    "* Third, determine whether the influential observation is valid, and only unusual. If you had a larger sample size, there might be more observations that are similar to the unusual one. You might need to collect more data to confirm the relationship that's suggested by the influential observation.\n",
    "* As a general rule, you should not exclude data. In many circumstances, some of the unusual observations contain important information. If you choose to exclude some observations, include in your report a description of the types of observations that were excluded and why. As part of your report or presentation, you should discuss the limitation of your conclusions, given the exclusions.\n",
    "\n",
    "### Collinearity\n",
    "We'd like to make sure that none of our predictors provides redundant information, that is, that there's no multicollinearity. Collinearity, also called multicollinearity, is a potential problem in multiple regression. It occurs when two or more predictor variables are highly correlated with each other.\n",
    "Because collinearity involving several predictors can be missed by correlations, we need additional tools for collinearity detection such as Variance Inflation Factors.\n",
    "\n",
    "Collinearity doesn't violate the assumptions of multiple regression. It means that there's redundant information among the predictor variables. \n",
    "\n",
    "#### Visualizing Collinearity\n",
    "You've seen that when variables are collinear, one of the variables provides nearly the same information as the other. X1 and X2 are collinear, so they follow a reasonably straight line. When the model includes both variables, neither one might be significant. However, when the model includes only one of them, either variable might be significant. This means that collinearity can hide significant effects, and this is a good reason to deal with collinearity before using any automated model selection tool. Second, collinearity increases the variance of the parameter estimates, which make them unstable. In turn, this increases the prediction error of the model\n",
    "This isn't an uncommon occurrence and illustrates an important point. If a composite variable is included in a model along with some or all of its component measures, there's bound to be collinearity. If the composite variable has meaning, it can be used as a substitute measure for both components, \n",
    "\n",
    "\n",
    "#### variance inflation factor (VIF)\n",
    "the important thing to remember is the approximate cutoff value. If the VIF is greater than 10 for any predictors in the model, those predictors are likely involved in collinearity.\n",
    "\n",
    "#### Using an Effective Modeling Cycle\n",
    "Now that you have a solid understanding of developing good regression models for your data, let's take a moment to review what you know about effective modeling.\n",
    "\n",
    "* First, you want to get to know your data by performing preliminary analyses. You should plot your data, calculate descriptive statistics, and perform correlation analysis.\n",
    "\n",
    "* Second, it's a good idea to check for collinearity before using any automated model selection techniques. This step can include the use of the correlation analyses and VIF statistics.\n",
    "\n",
    "* Third, you use PROC REG or PROC GLMSELECT to identify some good candidate models. No perfect model exists, so find the best, or simply, the most useful one. To narrow your choices to a few good models, you can use all-possible regressions if you have few candidate predictors, or stepwise selection methods when you consider many predictors.\n",
    "\n",
    "* Fourth is verifying model assumptions and searching for possible influential observations. You need to check and validate your assumptions by creating plots of residuals, and graphs of the residuals versus predicted values and predictors. To detect influential observations, you examine the RSTUDENT residuals, Cook's D statistic, DFFITS, and DFBETAS statistics.\n",
    "\n",
    "* Fifth, you need to revise your model if needed. If Steps 3 and 4 indicate the need for model revision, generate a new model by returning to these two steps. \n",
    "\n",
    "* The last step is prediction testing. \n",
    "\n",
    "You should try to validate your model with a holdout data set not used to build the model to see whether it generalizes well to new data sets. You'll learn about prediction testing, and predictive modeling in general, in the next lesson, when we move from inference to prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Building for Scoring and Prediction\n",
    "\n",
    "### Brief Introduction to Predictive Modeling\n",
    "\n",
    "\n",
    "#### Predictive Modeling Terminology\n",
    "**The goal** of predictive modeling is to predict, or score, future values of a target variable based on the existing values of inputs. \n",
    "\n",
    "The process: \n",
    "* begins with partitioning a data set into separate training and validation data sets. \n",
    "* The model is built using the training data and then assessed using the validation data. \n",
    "* After a best model is chosen, the model is deployed to make predictions on new data using a process called scoring.\n",
    "\n",
    "The predictor variables: \n",
    "* inputs, features, explanatory variables, or independent variables. \n",
    "\n",
    "The response variables: \n",
    "* targets, outcomes or dependent variables. \n",
    "\n",
    "The observations: \n",
    "* cases, instances or records.\n",
    "\n",
    "The measurement scale and variable type of the inputs and targets can be varied across different applications. They can be continuous variables  and categorical variables. They are often binary variables,. \n",
    "\n",
    "A predictive model consists of either a formula or rules, depending on the type of analysis that you use. \n",
    "**Formulas**: regression models, ... which are parametric and have formulas. \n",
    "**Rules** nonparametric models such as decision trees and random forests, which predict new cases based on a sequence of decisions, or rules, based on the values of the inputs.\n",
    "\n",
    "#### Model Complexity\n",
    "A model with just enough complexity, which also means just enough flexibility, gives the best generalization to new data sets. The important concept is that there isn't one perfect model. There's always a balance between overfitting and underfitting.\n",
    "\n",
    "\n",
    "Overfitting:  An overly complex model might be too flexible, accommodating nuances of the random noise or the chance relationships in the sample. Overfitting leads to models that have higher variance when they are applied to a population.\n",
    "\n",
    "\n",
    "Underfitting: an insufficiently complex model might not be flexible enough. This leads to underfitting. That is, systematically missing the signal, and underfitting leads to biased predictions. \n",
    "\n",
    "#### Building a Predictive Model\n",
    "The first part of the predictive modeling process is model building. You start by fitting a variety of models, and then you assess their performance and select the best model. You want to select a model that generalizes well that is, the model that's flexible enough to accurately predict new data. The key is to not overfit the training data set. The classic example of overfitting is selecting linear regression models based on R-square because the R-square metric improves or fits the model at least as well each time a predictor is added to the model.\n",
    "\n",
    "How can you select the best model?\n",
    "\n",
    "To ensure that the chosen model generalizes well, you use honest assessment, where you perform the assessment on a different data set than the one you use to build the model. Partitioning the data enables you to train the model and assess its performance on new cases. Using honest assessment, you partition or split the available data into a data set for training, and one for validation, and sometimes a third data set for testing. All partitions contain the predictors and the response.\n",
    "\n",
    "The training data set is used to fit a variety of different models. The validation data set is a holdout sample that's used to compare model performance and select the best performing model. Using a holdout sample is a way of assessing how well the models generalize to new data. If created, the test data set is used to give a final honest estimate of generalization for the chosen model. In practice, many analysts see no need for a final assessment. Instead, the model assessment measured on the validation data is reported as an upper bound on the performance that is expected when the model is deployed.\n",
    "\n",
    "Unfortunately, there's no globally optimal percentage to use when you partition the data. Common partitions include 70% training and 30% validation, or an 80/20 split, or 90/10 split, but the decision is the responsibility of the analyst.\n",
    "\n",
    "Predictive modeling is typically used when we have very large data sets and partitioning the data enables us to still build models on an adequate amount of data. However, if you start with a small or medium-size data set, partitioning the data might not be efficient, because the reduced sample size can severely degrade the fit of the model. In fact, computer-intensive methods, such as cross validation were developed so that all the data can be used for both fitting and honest assessment.\n",
    "\n",
    "#### Model Assessment and Selection\n",
    "During the model fitting phase of predictive modeling, you use the training data to create a variety of different models, and possibly use several model selection methods. In this example, we'll use the forward selection method to generate several candidate models. In forward selection, variables are added into the model as long as they meet the criterion for inclusion. For example, with the AICC criterion, variables will be added as long as the criterion value continues to decrease. When the value can no longer be reduced, indicating that the model can no longer be improved, the process stops.\n",
    "\n",
    "Here five possible models of increasing complexity were generated. For simplicity, the complexity of each model is indicated by a number from 1 to 5. Next, the validation data is used to assess the performance of each model. These performance measures are then used as the criteria for selecting the best model. In this simplified example, a star rating indicates how well the model fits the validation data. The validation assessment rating ranges from one star to three stars. Model 1 received a one-star rating, models 2 and 5 received two stars, and models 3 and 4 received three stars, indicating that they are the best models. But what if you need to choose only one?\n",
    "\n",
    "When there's a tie or a near tie, the most parsimonious model is typically chosen in other words, the simplest model with the highest validation assessment. Recall that, in this example, the models are listed in order of increasing complexity.This means that model 3 is simpler than model 4, so you choose model 3.\n",
    "\n",
    "\n",
    "\n",
    "#### Preparing for Scoring\n",
    "Before you start using a newly built model to score data, some preparation of the data might be required. For example, in database target marketing, the data to be scored might be many times larger than the data that is used to develop the model, and it might be stored in a different format. It's essential for the scoring data to be comparable to the training and validation data that were used to build the model.\n",
    "\n",
    "If any modifications were made to the data before the model was built and validated, then the same modifications must be made to the scoring data before scoring. This might include missing value imputation, transformations, and derivation of inputs through standardization or the creation of composite variables from existing variables. This is essential for the deployed model to be able to map the scoring functionality to the new data.\n",
    "\n",
    "Making the same modifications becomes more complex if the original modifications were based on parameters that were derived from the training data set, such as the mean or standard deviation. This means that if you subtracted the mean of a variable from the training and validation data, then the mean of that variable should also be subtracted from the scoring data. This practice keeps the different data sets comparable. The process of preparing the data for scoring can be time- and resource-intensive.\n",
    "\n",
    "#### Methods of Scoring\n",
    "When scoring, you don't rerun the algorithm that was used to build the model. Instead, you apply the score code, that is, the equations or rules obtained from the final model, to the new data. Let's look at three methods of scoring your data.\n",
    "\n",
    "The first uses a SCORE statement in PROC GLMSELECT to build and score a model in one step. This method is inefficient if you want to score more than once or use a large data set to build a model, because the model must be built from the training data each time the program is run.\n",
    "\n",
    "The second method uses a STORE statement in PROC GLMSELECT to build an item store, and a SCORE statement in PROC PLM. With this method, you build the model and an item store only once, and then access the item store in PROC PLM to score new data as often as needed. Separating the model building from scoring is especially helpful if your model is based on a very large training data set or if you want to score more than once. One potential problem with this method is that others might not be able to use this code with earlier versions of SAS.\n",
    "\n",
    "The third method uses a STORE statement in PROC GLMSELECT to build an item store, a CODE statement in PROC PLM to generate scoring code based on the item store, and a DATA step to run the scoring code. The resulting code is compatible with earlier versions of SAS, and you can share it with others without needing to share the item store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Categorical Data Analysis\n",
    "\n",
    "\n",
    "### Tests of Association\n",
    "\n",
    "We explored the distribution of our variables, Bonus, Fireplaces, Lot_Shape_2 and Basement_Area, and saw some possible associations of the three predictors with the response using crosstabulation tables and histograms. We need to assess whether the differences between the percentages of Bonus across levels of the predictors is greater than would be expected by chance. \n",
    "\n",
    "the chi-square test:\n",
    "* To be certain that the variables are associated.\n",
    "\n",
    "Cramer's V statistic and the Spearman correlation statistic:\n",
    "* To measure the magnitude of an association.\n",
    "\n",
    "#### The Pearson Chi-Square Test\n",
    "\n",
    "#### Mantel-Haenszel chi-square test\n",
    "The appropriate test to find **ordinal associations**. For ordinal associations, the Mantel-Haenszel chi-square test is a more powerful test than the Pearson chi-square test. The Mantel-Haenszel test considers only ordinal associations, whereas the Pearson test considers all possible associations.\n",
    "\n",
    "#### The Spearman Correlation Statistic\n",
    "To measure the **strength of the association between two ordinal variables**, you can use the Spearman correlation statistic. The Spearman correlation is a rank correlation, because it provides a degree of association between the ranks, or the levels, of the ordinal variables. This statistic has a range between -1 and 1. Values close to 1 indicate that thereâs a relatively high degree of positive correlation. Values close to -1 indicate that thereâs a relatively high degree of negative correlation. And values close to 0 indicate a weak correlation.\n",
    "\n",
    "Like other measures of association strength, Spearman's correlation statistic is not affected by sample size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Logistic Regression\n",
    "Logistic regression is used to model the relationship between a binary response and a set of predictor variables. Because the response is categorical, we estimate the probability of the response given the various categorical and continuous predictors.\n",
    "\n",
    "#### Modeling a Binary Response\n",
    "A linear regression model assumes the data is continuous, but for logistic regression, the response is binary. So, we can't simply use regression model assumptions, fitting techniques, and procedures we've learned thus far."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAS",
   "language": "sas",
   "name": "sas"
  },
  "language_info": {
   "codemirror_mode": "sas",
   "file_extension": ".sas",
   "mimetype": "text/x-sas",
   "name": "sas"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
